{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2nxXRV122D9T"
   },
   "source": [
    "\n",
    "### ***Implementing Policy and Value Iteration on the FrozenLake-v1 Environment***\n",
    "#  \n",
    "#### **Done by:**  Kishan Kanaiyalal Patel (200527734) \n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2T9K7nK4KY-I"
   },
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "\n",
    "import numpy as np         \n",
    "import gym                 # OpenAI gym for reinforcement learning environments\n",
    "import warnings            # Python warnings module for handling warning messages\n",
    "\n",
    "# Filter out warning messages to keep the output clean\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "CWj1GD4pKq3O",
    "outputId": "a42f1aca-8804-47f0-8917-a57a8a566399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space size: 4\n",
      "State space size: 16\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize the FrozenLake-v1 environment\n",
    "\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)  # Create the environment object\n",
    "env.reset()  # Reset the environment to the initial state\n",
    "\n",
    "# Get the number of possible states and actions in the environment\n",
    "n_states = env.observation_space.n   # Number of possible states\n",
    "n_actions = env.action_space.n       # Number of possible actions\n",
    "\n",
    "# Print the size of the action space and the number of possible states\n",
    "print(\"Action space size: {}\".format(n_actions))\n",
    "print(\"State space size: {}\".format(n_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xDnA6UCFMNZ7"
   },
   "outputs": [],
   "source": [
    "# Step 3: Policy evaluation function (Evaluates the value function for the given policy using iterative policy evaluation.)\n",
    "     \n",
    "def policy_evaluation(policy, gamma, smallest_change):\n",
    "    \n",
    "    # Initialize the value function for all states to 0\n",
    "    V = np.zeros(n_states)  \n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Initialize the change in the value function to 0      \n",
    "        delta = 0  \n",
    "\n",
    "        for s in range(n_states):  # Loop through all possible states\n",
    "            \n",
    "            # Store the previous value of the state\n",
    "            v = V[s]  \n",
    "\n",
    "            # Calculate the new value of the state using the Bellman equation for the given policy\n",
    "            V[s] = sum([p*(r + gamma*V[s_]) for p, s_, r, _ in env.P[s][policy[s]]])\n",
    "\n",
    "            # Update the change in the value function\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "\n",
    "        if delta < smallest_change:  # If the change is below the threshold, break out of the loop\n",
    "            break\n",
    "\n",
    "    return V  # Return the resulting value function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mShqt-ouahbd"
   },
   "outputs": [],
   "source": [
    "# Step 4: Policy improvement function (Improves the policy based on the value function)\n",
    "\n",
    "def policy_improvement(V, gamma):\n",
    "\n",
    "    # Create an array to hold the new policy\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    \n",
    "    # Iterate over all states in the environment\n",
    "    for s in range(n_states):\n",
    "      \n",
    "        # Compute the Q-values for all actions in the current state\n",
    "        q_values = [sum([p*(r + gamma*V[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(n_actions)]\n",
    "        \n",
    "        # Choose the action with the highest Q-value as the new action for the current state\n",
    "        policy[s] = np.argmax(q_values)\n",
    "    \n",
    "    # Return the new policy\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Y1RgptD7nw-s",
    "outputId": "214073ec-48ea-4e34-8885-9c29f4743716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration converged after 6 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Find the optimal policy using policy iteration\n",
    "\n",
    "gamma = 0.9  # set the discount factor\n",
    "smallest_change = 1e-10  # set the smallest change to consider for convergence\n",
    "n_iterations = 1000  # set the maximum number of iterations\n",
    "\n",
    "policy = np.zeros(n_states, dtype=int)  # initialize the policy\n",
    "\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    # evaluate the value function for the current policy\n",
    "    V = policy_evaluation(policy, gamma, smallest_change)  \n",
    "\n",
    "    # improve the policy based on the value function\n",
    "    new_policy = policy_improvement(V, gamma)  \n",
    "\n",
    "    if np.array_equal(policy, new_policy):  # check if the policy has converged\n",
    "        print(\"Policy iteration converged after %d iterations.\" % i)\n",
    "        break\n",
    "\n",
    "    policy = new_policy  # update the policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "6I85d-x5nz5t",
    "outputId": "0c869fbe-98a5-4f95-b254-374a262ef8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 4, Action: 1, Reward: 0.0, Total reward: 0.0\n",
      "State: 8, Action: 1, Reward: 0.0, Total reward: 0.0\n",
      "State: 9, Action: 2, Reward: 0.0, Total reward: 0.0\n",
      "State: 13, Action: 1, Reward: 0.0, Total reward: 0.0\n",
      "State: 14, Action: 2, Reward: 0.0, Total reward: 0.0\n",
      "State: 15, Action: 2, Reward: 1.0, Total reward: 1.0\n",
      "Episode finished\n",
      "\n",
      "Total reward with optimal policy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Performing the test pass on the environment \n",
    "\n",
    "# Defining a function to test a given policy\n",
    "def test_policy(policy):\n",
    "    total_reward = 0\n",
    "    state = env.reset() # Reset the environment and get the initial state\n",
    "\n",
    "    while True:\n",
    "        action = policy[state] # Select the action to take based on the policy\n",
    "        state, reward, done, _ = env.step(action) # Take the action and observe the next state and reward\n",
    "        total_reward += reward # Accumulate the reward\n",
    "\n",
    "        # Print out the current state, the action taken, the reward received and the total reward accumulated so far\n",
    "        print(f\"State: {state}, Action: {action}, Reward: {reward}, Total reward: {total_reward}\")\n",
    "\n",
    "        if done: # If the episode is finished, break the loop\n",
    "            print(\"Episode finished\")\n",
    "            break\n",
    "            \n",
    "    return total_reward\n",
    "\n",
    "# Test the optimal policy obtained from policy iteration and print the total reward\n",
    "total_reward = test_policy(policy)\n",
    "print(\"\\nTotal reward with optimal policy: \", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "fsZYFxecn3Hm",
    "outputId": "9412a7f5-c560-455e-bc87-c3f38927d487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0, Action: 3, Reward: 0.0, Total reward: 0.0\n",
      "State: 4, Action: 1, Reward: 0.0, Total reward: 0.0\n",
      "State: 8, Action: 1, Reward: 0.0, Total reward: 0.0\n",
      "State: 12, Action: 1, Reward: 0.0, Total reward: 0.0\n",
      "Episode finished\n",
      "\n",
      "Total reward with random policy:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Taking random steps\n",
    "\n",
    "# Define a function to test a random policy\n",
    "def random_policy_test():\n",
    "  \n",
    "    # Initialize total_reward to 0 and get the initial state of the environment\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    # Start an infinite loop until the episode is finished\n",
    "    while True:\n",
    "\n",
    "        # Choose a random action from the action space\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Take the chosen action and get the resulting state, reward, and whether the episode is finished\n",
    "        state, reward, done, _ = env.step(action) \n",
    "\n",
    "        # Update the total reward with the received reward\n",
    "        total_reward += reward\n",
    "\n",
    "        # Print out the current state, the action taken, the reward received and the total reward accumulated so far\n",
    "        print(f\"State: {state}, Action: {action}, Reward: {reward}, Total reward: {total_reward}\")\n",
    "        \n",
    "        # If the episode is finished, break the loop\n",
    "        if done:\n",
    "            print(\"Episode finished\")\n",
    "            break\n",
    "        \n",
    "    return total_reward\n",
    "\n",
    "# Call the random policy test function and print the total reward received\n",
    "total_reward = random_policy_test()\n",
    "print(\"\\nTotal reward with random policy: \", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2rc8q672n66C"
   },
   "outputs": [],
   "source": [
    "# Step 8: Perform value iteration\n",
    "\n",
    "# Define a function to perform value iteration\n",
    "def value_iteration(gamma, theta):\n",
    "\n",
    "    # Initialize the state value function to 0 for all states\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    # Start an infinite loop until convergence\n",
    "    while True:\n",
    "  \n",
    "        # Initialize the change in state values to 0\n",
    "        delta = 0\n",
    "        \n",
    "        # Update the value of each state\n",
    "        for s in range(n_states):\n",
    "\n",
    "            # Keep track of the old value of the state\n",
    "            v = V[s]\n",
    "            \n",
    "            # Calculate the Q-values for each action in the state\n",
    "            q_values = [sum([p*(r+gamma*V[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(n_actions)]\n",
    "            \n",
    "            # Set the value of the state to the maximum Q-value\n",
    "            V[s] = max(q_values)\n",
    "            \n",
    "            # Update the change in state values\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        \n",
    "        # If the change change in state values is less than the threshold, stop iterating\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Improve the policy based on the updated value function\n",
    "    policy = policy_improvement(V, gamma)\n",
    "    \n",
    "    # Return the optimal policy\n",
    "    return policy\n",
    "\n",
    "# Set the convergence threshold\n",
    "theta = 1e-8\n",
    "\n",
    "# Perform value iteration to obtain the optimal policy\n",
    "optimal_policy = value_iteration(gamma, theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "tu8R9EXaoAeU",
    "outputId": "dd78ff84-cbc5-421e-f0de-6af5f963d2d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 4, Action: 1, Reward: 0.0, Total reward: 0.0\n",
      "State: 8, Action: 1, Reward: 0.0, Total reward: 0.0\n",
      "State: 9, Action: 2, Reward: 0.0, Total reward: 0.0\n",
      "State: 13, Action: 1, Reward: 0.0, Total reward: 0.0\n",
      "State: 14, Action: 2, Reward: 0.0, Total reward: 0.0\n",
      "State: 15, Action: 2, Reward: 1.0, Total reward: 1.0\n",
      "Episode finished\n",
      "\n",
      "Total reward with optimal policy:  1.0\n",
      "\n",
      "\n",
      "State: 4, Action: 1, Reward: 0.0, Total reward: 0.0\n",
      "State: 0, Action: 3, Reward: 0.0, Total reward: 0.0\n",
      "State: 0, Action: 0, Reward: 0.0, Total reward: 0.0\n",
      "State: 1, Action: 2, Reward: 0.0, Total reward: 0.0\n",
      "State: 0, Action: 0, Reward: 0.0, Total reward: 0.0\n",
      "State: 1, Action: 2, Reward: 0.0, Total reward: 0.0\n",
      "State: 1, Action: 3, Reward: 0.0, Total reward: 0.0\n",
      "State: 0, Action: 0, Reward: 0.0, Total reward: 0.0\n",
      "State: 0, Action: 3, Reward: 0.0, Total reward: 0.0\n",
      "State: 4, Action: 1, Reward: 0.0, Total reward: 0.0\n",
      "State: 5, Action: 2, Reward: 0.0, Total reward: 0.0\n",
      "Episode finished\n",
      "\n",
      "Total reward with random policy:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Test the optimal policy\n",
    "total_reward = test_policy(optimal_policy)\n",
    "print(\"\\nTotal reward with optimal policy: \", total_reward)\n",
    "print('\\n')\n",
    "\n",
    "# Test the random policy\n",
    "total_reward = random_policy_test()\n",
    "print(\"\\nTotal reward with random policy: \", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Trq-fYisoCMM",
    "outputId": "dfe0fe17-8c5c-473f-f224-03e3d3c557b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value function:\n",
      "[0.59049 0.6561  0.729   0.6561  0.6561  0.      0.81    0.      0.729\n",
      " 0.81    0.9     0.      0.      0.9     1.      0.     ] \n",
      "\n",
      "Optimal policy:\n",
      "[1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal value function:\")\n",
    "print(V, '\\n')\n",
    "\n",
    "print(\"Optimal policy:\")\n",
    "print(optimal_policy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
